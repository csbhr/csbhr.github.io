<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>CDVD-TSP</title>

    <link rel="stylesheet" href="../css/normalize.css">
    <link rel="stylesheet" href="../css/project-page.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </head>
  <body>
    <div class="wrapper">
      <h1 class="center">Cascaded Deep Video Deblurring Using Temporal Sharpness Prior</h1>

      <p class="center">
        <a href="https://jspan.github.io/" target="_blank">Jinshan Pan</a>,
        <a href="https://csbhr.github.io/" target="_blank">Haoran Bai</a>,
        and Jinhui Tang
      </p>

      <p class="center"><img src="https://s1.ax1x.com/2020/03/31/GQnfpt.png" alt="Figure 1. Deblurred result on a real challenging video."></p>

      <h2>Abstract</h2>
      <hr>
      <p>
        We present a simple and effective deep convolutional neural network (CNN) model for video deblurring. The proposed algorithm mainly consists of optical flow estimation from intermediate latent frames and latent frame restoration steps. It first develops a deep CNN model to estimate optical flow from intermediate latent frames and then restores the latent frames based on the estimated optical flow. To better explore the temporal information from videos, we develop a temporal sharpness prior to constrain the deep CNN model to help the latent frame restoration. We develop an effective cascaded training approach and jointly train the proposed CNN model in an end-to-end manner. We show that exploring the domain knowledge of video deblurring is able to make the deep CNN model more compact and efficient. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods on the benchmark datasets as well as real-world videos.
      </p>
      <div class="fl m3">
        <h4>Downloads</h4>
        <ul>
          <li><a href="https://arxiv.org/abs/2004.02501" target="_blank">Paper</a></li>
          <li><a href="https://drive.google.com/drive/folders/1lw_1jITafEQ9DvMys_S6aYwtNApYKWsz?usp=sharing" target="_blank">Supplementary</a></li>
          <li><a href="https://github.com/csbhr/CDVD-TSP" target="_blank">Code</a></li>
        </ul>
      </div>
      <div class="fl m7">
        <h4>Citation</h4>
        <pre>@InProceedings{Pan_2020_CVPR,<br>  author = {Pan, Jinshan and Bai, Haoran and Tang, Jinhui},<br>  title = {Cascaded Deep Video Deblurring Using Temporal Sharpness Prior},<br>  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>  month = {June},<br>  year = {2020}<br>}</pre>
      </div>

      <hr>

      <h2>Proposed Algorithm</h2>
      <hr>
      <p class="center"><img src="https://s1.ax1x.com/2020/03/31/GQutu8.png" alt="Supp-Figure 1. An overview of the proposed method at one stage."></p>
      <p>
        The proposed algorithm contains the optical flow estimation module, latent image restoration module, and the temporal sharpness prior. For the optical flow estimation, we use the PWC-Net [5] to estimate optical flow. For the latent image restoration module, we use an encoder-decoder architecture based on [26]. However, we do not use the ConvLSTM module. Other network architectures are the same as [26].
      </p>
      <p>
        To effectively train the proposed algorithm, we develop a cascaded training approach and jointly train the proposed model in an end-to-end manner. At each stage, it takes three adjacent frames estimated from the previous stage as the input and generates the deblurred results of the central frame. When handling every three adjacent frames, the proposed network shares the same network parameters.
        The variables \( \tilde{I}_{i+1}(x) \) and \( \tilde{I}_{i-1}(x) \) denote the warped results of \( I_{i+1}(x+u_{i+1\rightarrow i}) \) and \( I_{i-1}(x+u_{i-1\rightarrow i}) \), respectively.
      </p>
      <h3>Temporal sharpness prior</h3>
      <p>
        As demonstrated in [4], the blur in the video is irregular, and thus there exist some pixels that are not blurred. Following the conventional method [4], we explore these sharpness pixels to help video deblurring. The sharpness prior is defined as:
        \[ S_i(x) = exp(-\frac{1}{2} \sum_{j\&j\neq0}D(I_{i+j}(x+u_{i+j \rightarrow i}); I_i(x))) \tag{10} \]
        where \( D(I_{i+j}(x+u_{i+j \rightarrow i}); I_i(x)) \) is defined as \( \left \| I_{i+j}(x+u_{i+j \rightarrow i} - I_i(x) \right \|^2 \).
      </p>
      <p>
        Based on (10), if the value of \( S_i(x) \) is close to 1, the pixel \(x\) is likely to be clear. Thus, we can use \( S_i(x) \) to help the deep neural network to distinguish whether the pixel is clear or not so that it can help the latent frame restoration. To increase the robustness of \( S_i(x) \), we define \( D(.) \) as:
        \[ D(I_{i+j}(x+u_{i+j \rightarrow i}); I_i(x)) = \sum_{y\in \omega(x)} \left \| I_{i+j}(x+u_{i+j \rightarrow i} - I_i(x) \right \|^2 \tag{11} \]
        where \( \omega(x) \) denotes an image patch centerd at pixel \( x \).
      </p>

      <h2>Quantitative Results</h2>
      <hr>
      <p>
        We further train the proposed method to convergence, and get higher PSNR/SSIM than the result reported in the paper.
      </p>
      <p>
        Quantitative results on the benchmark dataset by Su et al. [24]. All the restored frames instead of randomly selected 30 frames from each test set [24] are used for evaluations. <em>Note that: Ours* is the result that we further trained to convergence, and Ours is the result reported in the paper.</em>
      </p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQOAv6.png" alt="Table 1. Quantitative evaluations on the video deblurring dataset [24] in terms of PSNR and SSIM."></p>
      <p>
        Quantitative results on the GOPRO dataset by Nah et al.[20].
      </p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQYZi8.png" alt="Table 2. Quantitative evaluations on the video deblurring dataset [20] in terms of PSNR and SSIM."></p>

      <h2>Visual Comparisons</h2>
      <hr>
      <p>
        Here are some visual comparisons with state-of-the-art methods. The proposed algorithm generates much clearer frames. More visual results are included in <a href="https://drive.google.com/drive/folders/1lw_1jITafEQ9DvMys_S6aYwtNApYKWsz?usp=sharing" target="_blank">supplementary materials</a>.
      </p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQt01g.png" alt="Figure 2. Deblurred results on the test dataset [24]."></p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQtgA0.png" alt="Figure 3. Deblurred results on the test dataset [20]."></p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQtf9U.png" alt="Figure 4. Deblurred results on a real video from [4]."></p>
      <p class='center'><img src="https://s1.ax1x.com/2020/04/02/GJQ7aq.png" alt="Supp-Figure 8. Deblurred results on the test dataset [3]."></p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQrpkj.png" alt="Supp-Figure 7. Deblurred results on the test dataset [4]."></p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQD6YR.png" alt="Supp-Figure 6. Deblurred results on the test dataset [4]."></p>

      <h2>Contact</h2>
      <hr>
      <p>If you have any question, please contact us by:</p>
      <ul>
        <li>E-mail: baihaoran@njust.edu.cn</li>
        <li>Github Issue: https://github.com/csbhr/CDVD-TSP/issues</li>
      </ul>

    </div>

  </body>
</html>
